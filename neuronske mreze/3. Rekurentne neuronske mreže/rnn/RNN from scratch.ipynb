{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6192fcb0-a2fc-4b4e-b7e2-63e747e93a78",
   "metadata": {},
   "source": [
    "# Rekurentne neuronske mreže"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d38fe8d-19d2-483f-96df-e42fc6d66808",
   "metadata": {},
   "source": [
    "Primer implementacije proste rekurentne neuronske mreže (RNN) *from scratch*, njeno treniranje na spisku imena dinosaurusa i primena za generisanje novog imena dinosaurusa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c55894-9f2e-4da4-ac2a-bbae89eaad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bbd2fc-5f68-4840-bd3e-8eaa1106dce4",
   "metadata": {},
   "source": [
    "RNN su tip arhitekture koji je dobro prilagođen za obradu sekvencijalnih podataka kao što su tekst, audio, vremenske serije, itd. Ključni aspekt koji ih čini jedinstvenim je prisustvo rekurzivnih računarskih petlji koje obuhvataju susedne vremenske korake. Ovo ponavljanje omogućava modelima RNN da efikasno održavaju trajno interno stanje ili \"memoriju\" prethodnih ulaza koji bi mogli da informišu obradu podataka kasnije u sekvenci.\n",
    "\n",
    "Vizuelno, tipična RNN struktura se sastoji od jedne računarske jedinice sa samopovezanim skrivenim stanjem, kroz koje informacije kruže kroz vremenske korake:\n",
    "\n",
    "![Odmotavanje](images/unfold.png)\n",
    "\n",
    "Kako podaci prolaze kroz RNN, aktivacije iz prethodnih vremenskih koraka teku kao ulazi na proračune mreže na trenutne podatke. Ovo omogućava modelu da dinamički inkorporira vremenski kontekst i istoriju sekvence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b93087-97ba-491f-8a16-8538802d6b40",
   "metadata": {},
   "source": [
    "## Odmotavanje RNN\n",
    "\n",
    "Umesto da posmatramo samo sažetu vizuelizaciju mreže, njeno odmotavanje oktriva ponavljajuću prirodu RNN izračunavanja, pretvarajući ga u tradicionalniniji usmereni aciklični računarski graf koji se sastoji od više vremenskih koraka. Tada možemo vizuelno videti kako se ulazna sekvenca obrađuje u svim vremenskim koracima, što pomaže razumevanju izračunavanja unapred i unazad kroz vreme kako bismo kasnije trenirali parametre.\n",
    "\n",
    "![Forward](images/forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae863a4-0141-46d4-9a91-743ccca2b74c",
   "metadata": {},
   "source": [
    "## *Forward pass*\n",
    "\n",
    "*Forward pass* je mesto gde RNN obrađuje ulazne sekvence korak po korak kako bi računao predikcije.\n",
    "\n",
    "1. **Računanje skrivenog stanja na osnovu ulaza**:\n",
    "\n",
    "Za svaki vremenski korak $t$ u sekvenci, računamo skriveno stanje $h_t$ kao:  \n",
    "$$ h_t = \\tanh(U \\cdot x_t + W \\cdot h_{t-1} + b_h) $$  \n",
    "gde:\n",
    "- $U$ je matrica težina koja povezuje ulazni sloj sa skrivenim slojem\n",
    "- $W$ je matrica težina koja povezuje prethodno skriveno stanje sa trenutnim skrivenim stanjem\n",
    "- $b_h$ je *bias* vektor za skriveni sloj\n",
    "- $tanh$ je aktivaciona funkcija koja uvodi nelinearnost. \n",
    "\n",
    "2. **Računanje izlaza na osnovu skrivenog stanja**:\n",
    "\n",
    "Izlaz $y_t$ u svakom vremenskom koraku se računa kao:  \n",
    "$$ y_t = \\text{softmax}(V \\cdot h_t + b_y) $$  \n",
    "gde:  \n",
    "- $V$ je matrica težina koja povezuje skriveni sloj sa izlaznim slojem\n",
    "- $b_y$ je *bias* vektor za izlazni sloj\n",
    "- $softmax$ normalizuje izlaze na distribuciju verovatnoće preko mogućih sledećih znakova.\n",
    "\n",
    "3. **Čuvanje izlaza i skrivenih stanja**:\n",
    "\n",
    "Čuvamo svako skriveno stanje $h_t$ i izlaz $y_t$ za upotrebu u *backward pass*-u.  \n",
    "\n",
    "### Tok podataka\n",
    "\n",
    "Na slici ispod vidimo primer toka podataka - pokušavamo da predvidimo sledeći znak na osnovu nekog niza znakova.\n",
    "\n",
    "![Tok](images/flow.png)  \n",
    "\n",
    "Ulazi su diskretni karakteri, koji se konvertuju u *one-hot* vektore. Skriveni sloj i izlazni sloj vrše proračune kao što je pomenuto u prethodnim formulama. Na osnovu izlaza *softmax* funkcije, možemo to pretvorditi nazad u prediktovanu reč. Svaki vremenski korak takođe ima prediktovani izlaz. $y_t$ u poslednjem vremenskom koraku je konačno predviđanje sledećeg karaktera uzlazne sekvence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea1334-ceea-44c1-b9e6-7d735ec66f0e",
   "metadata": {},
   "source": [
    "## *Backward pass*\n",
    "\n",
    "Koristimo propagaciju u nazad kroz vreme (eng. *Backpropagation Through Time*).\n",
    "Za odmotani računarski graf, slično onome kako sekvenca teče kroz mrežu sloj po sloj u *forward pass*-u, računamo gradijente u *backward pass*-u sloj po sloj, ali od poslednjeg vremenskog koraka ka početnom koraku, otuda i naziv \"kroz vreme\". *Loss* funkcija je unakrsna entropija (eng. *cross-entropy*), s tim što je proširujemo sa malom konstantom $1e-8$ koja će sprečiti greške u računanju.\n",
    "\n",
    "![Backward pass](images/backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4cc737-b682-48c1-a23d-ffc209ee9fbc",
   "metadata": {},
   "source": [
    "Računamo gradijente za svaku matricu težina kako bi unapredili model na osnovu grešaka predikcije:\n",
    "\n",
    "1. **Greška izlaznog sloja**:\n",
    "\n",
    "Greška na konačnom izlazu se računa kao:  \n",
    "$$\\delta_y = y_{\\text{pred}} - y_{\\text{true}}$$  \n",
    "gde:\n",
    "- $y_{\\text{pred}}$ je prediktovana verovatnoća\n",
    "- $y_{\\text{true}}$ je *one-hot* enkodovana tačna vrednost.\n",
    "\n",
    "Gradijenti za izlazni sloj:\n",
    "$$\\frac{\\partial \\text{Loss}}{\\partial V} = \\delta_y \\cdot h_t^T$$\n",
    "$$\\frac{\\partial \\text{Loss}}{\\partial b_y} = \\delta_y$$\n",
    "\n",
    "2. **Greška skrivenog sloja**:\n",
    "\n",
    "Propagiramo grešku unazad kroz vreme da prilagodimo $W$, $U$ i $b_h$ za svaki prethodni vremenski korak. Za svaki korak $t$ računamo:  \n",
    "$$\\delta_h = (\\delta_y \\cdot V^T + \\delta_h^{\\text{next}}) \\odot (1 - h_t^2)$$  \n",
    "gde:  \n",
    "- $\\delta_h^{\\text{next}}$ je greška iz sledećeg vremenskog koraka\n",
    "- $(1 - h_t^2)$ je izvod $tanh$ funkcije.\n",
    "\n",
    "Gradijenti za skriveni sloj:\n",
    "$$\\frac{\\partial \\text{Loss}}{\\partial U} += \\delta_h \\cdot x_t^T$$\n",
    "$$\\frac{\\partial \\text{Loss}}{\\partial W} += \\delta_h \\cdot h_{t-1}^T$$\n",
    "$$\\frac{\\partial \\text{Loss}}{\\partial b_h} += \\delta_h$$\n",
    "\n",
    "3. **Ažuriranje težina**:\n",
    "\n",
    "Nakon izračunavanja gradijenata u svim vremenskim koracima, ažuriramo svaki parametar na sledeći način:  \n",
    "$$U = U - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial U}$$  \n",
    "$$W = W - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial W}$$  \n",
    "$$V = V - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial V}$$  \n",
    "$$b_h = b_h - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial b_h}$$  \n",
    "$$b_y = b_y - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial b_y}$$  \n",
    "gde je $\\eta$ stopa učenja (eng. *learning rate*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8314c9-8fbe-4d77-9733-1f1d0f8790ea",
   "metadata": {},
   "source": [
    "## Implementacija\n",
    "\n",
    "### Mreža\n",
    "\n",
    "Implementacija klase koja predstavlja jednostavnu RNN sa metodama za *forward pass*, *backward pass*, treniranje i predikciju."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c939e6-1392-4390-8b8a-75616dcc4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # inicijalizacija matrica težina sa malim nasumičnim vrednostima\n",
    "        self.U = np.random.uniform(-0.01, 0.01, (hidden_size, input_size))  # ulaz ka skrivenom\n",
    "        self.W = np.random.uniform(-0.01, 0.01, (hidden_size, hidden_size))  # skriveni ka skrivenom\n",
    "        self.V = np.random.uniform(-0.01, 0.01, (output_size, hidden_size))  # skriveni ka izlazu\n",
    "        # inicijalizacija bias-a\n",
    "        self.b_h = np.zeros((hidden_size, 1))  # bias za skriveni sloj\n",
    "        self.b_y = np.zeros((output_size, 1))  # bias za izlazni sloj\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.hidden_states = [np.zeros((self.W.shape[0], 1))]  # inicijalizacija skrivenog stanja\n",
    "        self.outputs = [] # inicijalizacija liste za čuvanje predikcija u svakom vremenskom koraku\n",
    "\n",
    "        # prolazak kroz svaki vremenski korak u ualznoj sekvenci\n",
    "        for x_t in inputs:\n",
    "            # novo skriveno stanje\n",
    "            h_t = np.tanh(self.U @ x_t + self.W @ self.hidden_states[-1] + self.b_h)\n",
    "            # novi izlaz\n",
    "            y_t = self.softmax(self.V @ h_t + self.b_y)\n",
    "            # cuvanje skrivenog stanja i izlaza u trenutnom koraku\n",
    "            self.hidden_states.append(h_t)\n",
    "            self.outputs.append(y_t)\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, inputs, target, learning_rate = 0.01):\n",
    "        # inicijalizacija gradijenata za matrice težina\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db_h, db_y = np.zeros_like(self.b_h), np.zeros_like(self.b_y)\n",
    "        delta_h_next = np.zeros_like(self.hidden_states[0])\n",
    "\n",
    "        # računanje greške izlaznog sloja u poslednjem vremenskom koraku\n",
    "        delta_y = self.outputs[-1] - target\n",
    "        dV += delta_y @ self.hidden_states[-1].T # gradijent izlazne matrice težina\n",
    "        db_y += delta_y # gradijent za izlazni bias\n",
    "\n",
    "        # BPTT \n",
    "        for t in reversed(range(len(inputs))):\n",
    "            # greška skrivenog sloja\n",
    "            delta_h = (self.V.T @ delta_y + delta_h_next) * (1 - self.hidden_states[t + 1] ** 2)\n",
    "            # akumulacija gradijenata\n",
    "            dU += delta_h @ inputs[t].T\n",
    "            dW += delta_h @ self.hidden_states[t].T\n",
    "            db_h += delta_h\n",
    "            # ažuriranje za sledeći vremenski korak u BP petlji\n",
    "            delta_h_next = self.W.T @ delta_h\n",
    "\n",
    "        # ažuriranje teina i bias-ova sa akumuliranim gradijentima\n",
    "        self.U -= learning_rate * dU\n",
    "        self.W -= learning_rate * dW\n",
    "        self.V -= learning_rate * dV\n",
    "        self.b_h -= learning_rate * db_h\n",
    "        self.b_y -= learning_rate * db_y\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x, axis=0)\n",
    "\n",
    "    def fit(self, inputs, targets, epochs = 1000, learning_rate = 0.01, verbose=0):\n",
    "        # inicijalizacija liste za čuvanje loss-a za svaku epohu\n",
    "        loss_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            # procesiranje svake sekvence u trening skupu\n",
    "            for i in range(len(inputs)):\n",
    "                x_seq = inputs[i]\n",
    "                y_true = targets[i]\n",
    "                \n",
    "                # forward pass za predikcije\n",
    "                outputs = self.forward(x_seq)\n",
    "                y_pred = outputs[-1] # uzimamo poslednju predikciju iz sekvence\n",
    "\n",
    "                # računanje unakrsne entropije za sekvencu\n",
    "                loss = -np.sum(y_true * np.log(y_pred + 1e-8))\n",
    "                epoch_loss += loss # akumulacija loss-a za epohu\n",
    "\n",
    "                # backward pass za ažuriranje težina\n",
    "                self.backward(x_seq, y_true, learning_rate)\n",
    "            \n",
    "            loss_history.append(epoch_loss)\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # vizuelizacija loss-a\n",
    "        plt.plot(loss_history)\n",
    "        plt.title(\"Loss during training\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, start_sequence, char_to_index, index_to_char, max_length = 20):\n",
    "        # inicijalizacija generisanog imena navedenom početnom sekvencom\n",
    "        name = start_sequence\n",
    "        # one-hot početne sekvence karaktera za ulaz u RNN\n",
    "        input_seq = [self.one_hot_encode(char, len(char_to_index), char_to_index) for char in start_sequence]\n",
    "\n",
    "        for _ in range(max_length - len(start_sequence)):\n",
    "            # forward pass koristeći poslednji sequence_length karakter za predikciju\n",
    "            output = self.forward(input_seq[-sequence_length:])[-1]\n",
    "            # uzimanje indeksa karaktera sa najvećom verovatnoćom\n",
    "            predicted_index = np.argmax(output)\n",
    "            # konverzija indeksa u karakter\n",
    "            predicted_char = index_to_char[predicted_index]\n",
    "        \n",
    "            # prekidanje generisanja ako se dođe do znaka za novu liniju\n",
    "            if predicted_char == \"\\n\":\n",
    "                break\n",
    "\n",
    "            # dodaj karakter na generisano ime\n",
    "            name += predicted_char\n",
    "            # dodaj one-hot enkoding prediktovanog karaktera za sledeći vremenski korak\n",
    "            input_seq.append(self.one_hot_encode(predicted_char, len(char_to_index), char_to_index))\n",
    "    \n",
    "        return name\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encode(char, vocab_size, char_to_index):\n",
    "        # generisanje one-hot vektora za dati karakter\n",
    "        vector = np.zeros((vocab_size, 1))\n",
    "        vector[char_to_index[char]] = 1\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89dc07e-dc9f-4a1b-bde7-82f3f2493a3a",
   "metadata": {},
   "source": [
    "### Trening podaci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421e3f8-a72d-436b-a8d2-6a6529a78079",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dinos.txt\", \"r\") as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe9554-68fa-4c28-ac86-a54528f6c9c6",
   "metadata": {},
   "source": [
    "### Priprema vokabulara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564b210-1778-4deb-9dc6-0dbb798c368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(\"\".join(names)))\n",
    "print(len(vocab))\n",
    "print(vocab)\n",
    "char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
    "index_to_char = {idx: char for char, idx in char_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df74fa-754a-467e-af14-1ee8ce16c8df",
   "metadata": {},
   "source": [
    "### Priprema parova ulaz-izlaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f0e1e-0996-4f24-87ce-65357de775d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "inputs, targets = [], []\n",
    "for name in names:\n",
    "    encoded_name = [RNN.one_hot_encode(char, len(char_to_index), char_to_index) for char in name]\n",
    "    for i in range(len(encoded_name) - sequence_length):\n",
    "        inputs.append(encoded_name[i:i + sequence_length]) # ulazna sekvenca\n",
    "        targets.append(encoded_name[i + sequence_length]) # sledeći karakter\n",
    "\n",
    "print(len(inputs))\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a706191c-e576-4961-9773-2a06d86ca7e5",
   "metadata": {},
   "source": [
    "### Inicijalizacija modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c3a98-914e-4ff1-b3b4-75010fa7ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(char_to_index)\n",
    "hidden_size = 50\n",
    "output_size = len(char_to_index)\n",
    "rnn = RNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f8700-1691-443a-b494-a9c23d9b8d47",
   "metadata": {},
   "source": [
    "### Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0449f5-0fe0-4749-b4ff-bffd361227f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.fit(inputs, targets, epochs=20, learning_rate=0.01, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241754b-2311-487d-a272-819981699980",
   "metadata": {},
   "source": [
    "### Predikcija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50daf477-ddee-4bf7-9130-f2708379413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated Dinosaur Name:\", rnn.predict(\"A\", char_to_index, index_to_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618242e-46bd-4072-bdd7-cb236f4fb1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
